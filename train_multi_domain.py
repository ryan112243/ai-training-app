"""
ç›´æ¥åŸ·è¡Œå¤šé ˜åŸŸæ¨¡å‹è¨“ç·´è…³æœ¬
"""

import torch
import torch.nn as nn
import torch.optim as optim
import os
import json
from datetime import datetime
from multi_domain_loader import MultiDomainDataProcessor
from models import SimpleNN
from config import Config

class MultiDomainTrainer:
    """å¤šé ˜åŸŸæ¨¡å‹è¨“ç·´å™¨"""
    
    def __init__(self, config=None):
        self.config = config or Config()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è¨­å‚™: {self.device}")
        
    def train_model(self, data_paths, epochs=5, batch_size=2, learning_rate=0.001):
        """
        è¨“ç·´å¤šé ˜åŸŸæ¨¡å‹
        
        Args:
            data_paths: æ•¸æ“šè·¯å¾‘å­—å…¸
            epochs: è¨“ç·´è¼ªæ•¸
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¸ç¿’ç‡
        """
        try:
            print("é–‹å§‹å¤šé ˜åŸŸæ¨¡å‹è¨“ç·´...")
            
            # å‰µå»ºæ•¸æ“šè™•ç†å™¨
            processor = MultiDomainDataProcessor(self.config)
            
            # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
            dataloaders = processor.create_dataloaders(
                data_paths, 
                batch_size=batch_size,
                train_split=0.8
            )
            
            if not dataloaders:
                print("éŒ¯èª¤: æ²’æœ‰å¯ç”¨çš„æ•¸æ“šåŠ è¼‰å™¨")
                return None
            
            # å‰µå»ºæ¨¡å‹
            model = SimpleNN(
                input_size=512,  # å‡è¨­è¼¸å…¥ç‰¹å¾µç¶­åº¦
                hidden_size=256,
                output_size=512,  # ä¿®æ”¹è¼¸å‡ºç¶­åº¦èˆ‡è¼¸å…¥åŒ¹é…
                num_classes=512  # é ˜åŸŸæ•¸é‡
            ).to(self.device)
            
            # å®šç¾©æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨
            criterion = nn.MSELoss()
            optimizer = optim.Adam(model.parameters(), lr=learning_rate)
            
            # è¨“ç·´å¾ªç’°
            training_history = []
            
            for epoch in range(epochs):
                epoch_loss = 0.0
                batch_count = 0
                
                print(f"\nç¬¬ {epoch + 1}/{epochs} è¼ªè¨“ç·´:")
                
                # éæ­·æ‰€æœ‰è¨“ç·´æ•¸æ“šåŠ è¼‰å™¨
                for loader_name, loader in dataloaders.items():
                    if 'train' not in loader_name:
                        continue
                        
                    domain = loader_name.replace('_train', '')
                    print(f"  è¨“ç·´ {domain} é ˜åŸŸ...")
                    
                    for batch_idx, batch in enumerate(loader):
                        # ç°¡åŒ–çš„ç‰¹å¾µæå–ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­éœ€è¦æ›´è¤‡é›œçš„è™•ç†ï¼‰
                        inputs = self._extract_features(batch['inputs'])
                        targets = self._extract_features(batch['outputs'])
                        
                        inputs = inputs.to(self.device)
                        targets = targets.to(self.device)
                        
                        # å‰å‘å‚³æ’­
                        optimizer.zero_grad()
                        outputs = model(inputs)
                        
                        # è¨ˆç®—æå¤±
                        loss = criterion(outputs, targets)
                        
                        # åå‘å‚³æ’­
                        loss.backward()
                        optimizer.step()
                        
                        epoch_loss += loss.item()
                        batch_count += 1
                        
                        if batch_idx % 10 == 0:
                            print(f"    æ‰¹æ¬¡ {batch_idx}, æå¤±: {loss.item():.4f}")
                
                avg_loss = epoch_loss / max(batch_count, 1)
                training_history.append({
                    'epoch': epoch + 1,
                    'loss': avg_loss
                })
                
                print(f"  å¹³å‡æå¤±: {avg_loss:.4f}")
            
            # ä¿å­˜æ¨¡å‹ï¼ˆæœªåŠ å¯†ï¼‰
            model_path = self._save_model(model, training_history)
            
            print(f"\nè¨“ç·´å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
            return model_path
            
        except Exception as e:
            print(f"è¨“ç·´éç¨‹ä¸­å‡ºéŒ¯: {e}")
            return None
    
    def _extract_features(self, texts):
        """
        ç°¡åŒ–çš„ç‰¹å¾µæå–ï¼ˆå°‡æ–‡æœ¬è½‰æ›ç‚ºæ•¸å€¼ç‰¹å¾µï¼‰
        å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰è©²ä½¿ç”¨æ›´è¤‡é›œçš„æ–‡æœ¬ç·¨ç¢¼æ–¹æ³•
        """
        features = []
        for text in texts:
            # ç°¡å–®çš„å­—ç¬¦ç´šç‰¹å¾µæå–
            feature = [ord(c) % 256 for c in text[:512]]
            # å¡«å……æˆ–æˆªæ–·åˆ°å›ºå®šé•·åº¦
            if len(feature) < 512:
                feature.extend([0] * (512 - len(feature)))
            else:
                feature = feature[:512]
            features.append(feature)
        
        return torch.FloatTensor(features)
    
    def _save_model(self, model, training_history):
        """ä¿å­˜æ¨¡å‹ï¼ˆæœªåŠ å¯†æ ¼å¼ï¼‰"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir = "models/multi_domain"
        os.makedirs(model_dir, exist_ok=True)
        
        model_path = os.path.join(model_dir, f"multi_domain_model_{timestamp}.pth")
        
        # ä¿å­˜å®Œæ•´çš„æ¨¡å‹ç‹€æ…‹ï¼ˆæœªåŠ å¯†ï¼‰
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_architecture': {
                'input_size': 512,
                'hidden_size': 256,
                'output_size': 512,
                'num_classes': 512
            },
            'training_history': training_history,
            'timestamp': timestamp,
            'pytorch_version': torch.__version__
        }, model_path)
        
        # åŒæ™‚ä¿å­˜ç‚ºå¯è®€çš„JSONæ ¼å¼é…ç½®
        config_path = os.path.join(model_dir, f"model_config_{timestamp}.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump({
                'model_path': model_path,
                'architecture': {
                    'input_size': 512,
                    'hidden_size': 256,
                    'output_size': 512,
                    'num_classes': 512
                },
                'training_history': training_history,
                'timestamp': timestamp,
                'usage_instructions': {
                    'load_model': f"torch.load('{model_path}')",
                    'description': "æœªåŠ å¯†çš„PyTorchæ¨¡å‹ï¼Œå¯ç›´æ¥åœ¨å…¶ä»–ç¨‹å¼ä¸­è¼‰å…¥ä½¿ç”¨"
                }
            }, f, ensure_ascii=False, indent=2)
        
        print(f"æ¨¡å‹é…ç½®å·²ä¿å­˜è‡³: {config_path}")
        return model_path

def main():
    """ä¸»å‡½æ•¸ - ç›´æ¥åŸ·è¡Œè¨“ç·´"""
    print("=== å¤šé ˜åŸŸAIæ¨¡å‹ç›´æ¥è¨“ç·´ ===")
    
    # æ•¸æ“šè·¯å¾‘é…ç½®
    data_paths = {
        "math": "data/samples/math_sample.json",
        "programming": "data/samples/programming_sample.json",
        "dialogue": "data/samples/dialogue_sample.json"
    }
    
    # æª¢æŸ¥æ•¸æ“šæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    missing_files = []
    for domain, path in data_paths.items():
        if not os.path.exists(path):
            missing_files.append(f"{domain}: {path}")
    
    if missing_files:
        print("éŒ¯èª¤: ä»¥ä¸‹æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨:")
        for file in missing_files:
            print(f"  - {file}")
        print("\nè«‹å…ˆå‰µå»ºç¤ºä¾‹æ•¸æ“šæˆ–æª¢æŸ¥æ–‡ä»¶è·¯å¾‘")
        return
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = MultiDomainTrainer()
    
    # é–‹å§‹è¨“ç·´
    model_path = trainer.train_model(
        data_paths=data_paths,
        epochs=3,  # è¼ƒå°‘çš„è¼ªæ•¸ç”¨æ–¼å¿«é€Ÿæ¸¬è©¦
        batch_size=1,  # å°æ‰¹æ¬¡é©åˆç¤ºä¾‹æ•¸æ“š
        learning_rate=0.001
    )
    
    if model_path:
        print(f"\nâœ… è¨“ç·´æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶: {model_path}")
        print(f"ğŸ”“ æ¨¡å‹æœªåŠ å¯†ï¼Œå¯ç›´æ¥åœ¨å…¶ä»–ç¨‹å¼ä¸­ä½¿ç”¨")
        print(f"\nä½¿ç”¨æ–¹æ³•:")
        print(f"import torch")
        print(f"model_data = torch.load('{model_path}')")
        print(f"model_state = model_data['model_state_dict']")
    else:
        print("âŒ è¨“ç·´å¤±æ•—")

if __name__ == "__main__":
    main()